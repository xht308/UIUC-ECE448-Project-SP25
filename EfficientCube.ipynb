{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46d128b",
   "metadata": {},
   "source": [
    "# EfficientCube+: DNN-based Rubik's Cube Solver\n",
    "\n",
    "## Overview\n",
    "The standalone notebook serves as a demostration of the methods proposed in our ECE448 Final Project @ UIUC in SP2025 and provides the necessary code to reproduce the experiment.\n",
    "\n",
    "## Related work\n",
    "The work is based on the the following publiciation:\n",
    "> K. Takano. Self-Supervision is All You Need for Solving Rubik's Cube. Transactions on Machine Learning Research, ISSN 2835-8856, 2023. URL: https://openreview.net/forum?id=bnBeNFB27b.\n",
    "\n",
    "## Environment Reference\n",
    "The notebook is designed to be run and tested on [Illinois Computes Research Notebooks](http://go.ncsa.illinois.edu/jupyter) (ICRN), which is equipped with the following resources:\n",
    "- AMD EPYC-Milan Processor Core * 2\n",
    "- 8GB of RAM\n",
    "- NVIDIA A100-SXM4-80GB (shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbdc4b0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Due to the short time period of the project and the limited resources, the default training and searching configuration is sub-optimal aimed to achieve a balance between time/resource consumption and the performance of the resulting model.\n",
    "\n",
    "To make comparsion to and reproduce the best-reported result and in the original [EfficientCube](https://github.com/kyo-takano/efficientcube) project, which we refered to, it is suggested to set `TrainConfig.num_steps = 2000000` and `SearchConfig.beam_width = 2**18`.\n",
    "\n",
    "To accelerate training and inference, the mixed precision mode can be enabled by setting `ENABLE_FP16` to `True` with possible minor performance degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3476c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    max_depth = 26                          # God's Number\n",
    "    batch_size_per_depth = 1000\n",
    "    num_steps = 10000\n",
    "    learning_rate = 1e-3\n",
    "    INTERVAL_PLOT, INTERVAL_SAVE = 100, 1000\n",
    "    ENABLE_FP16 = False                     # Set this to True if you want to train the model faster\n",
    "\n",
    "class SearchConfig:\n",
    "    beam_width = 2**11                      # This controls the trade-off between time and optimality\n",
    "    max_depth = TrainConfig.max_depth * 2   # Any number above God's Number will do\n",
    "    ENABLE_FP16 = False                     # Set this to True if you want to solve faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9abf6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from cycler import cycler\n",
    "from IPython.display import clear_output\n",
    "from torch import nn\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Set the default color cycle for matplotlib plots\n",
    "plt.rcParams[\"axes.prop_cycle\"] = cycler(color=[\"#000000\", \"#2180FE\", \"#EB4275\"])\n",
    "\n",
    "# Enable TensorFloat32 (TF32) Training/Inference on Ampere (or higher) GPUs\n",
    "# Supported on Nvidia A100 (test environment)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ededb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "os.cpu_count(): 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic information\n",
    "print(f'device: {device}')\n",
    "print(f'os.cpu_count(): {os.cpu_count()}')\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7e75d",
   "metadata": {},
   "source": [
    "## Rubik's Cube\n",
    "The Rubik's Cube is represented and operated based on the locations and color labels of $6\\times3\\times3$ stickers.\n",
    "\n",
    "In this work, the [Quarter-Turn Metric](https://www.speedsolving.com/wiki/index.php/Metric#QTM) (90° turns count as one move; 180°, two) is employed for the movement of the cube.\n",
    "\n",
    "Compared to the original implementation, our implementation makes full use of the **Vector and SIMD operation**, making the cube suitable to be operated on GPUs and achieved significant performance improvement on both training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd72528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cubes:\n",
    "    \"\"\"\n",
    "    A class for a set of 3x3x3 Rubik's cubes.\n",
    "\n",
    "    Each cube is represented as a 1D tensor of shape (6 * 3 * 3,).\n",
    "    Initial color:\n",
    "\n",
    "            0 0 0\n",
    "            0 Y 0\n",
    "            0 0 0\n",
    "\n",
    "    2 2 2   5 5 5   3 3 3  4 4 4\n",
    "    2 B 2   5 R 5   3 G 3  4 O 4\n",
    "    2 2 2   5 5 5   3 3 3  4 4 4\n",
    "\n",
    "            1 1 1\n",
    "            1 W 1\n",
    "            1 1 1\n",
    "    \n",
    "    Indices of state (each starting with 9*(n-1)):\n",
    "\n",
    "                 2   5   8\n",
    "                 1   4   7\n",
    "                [0]  3   6\n",
    "     20  23 26  47  50  53  29  32 35  38  41 44\n",
    "     19  22 25  46  49  52  28  31 34  37  40 43\n",
    "    [18] 21 24 [45] 48  51 [27] 30 33 [36] 39 42\n",
    "                11   14 17\n",
    "                10   13 16\n",
    "                [9]  12 15\n",
    "    \"\"\"\n",
    "\n",
    "    ## Class variables ##\n",
    "\n",
    "    # Initialization indicator\n",
    "    __initialized: bool = False\n",
    "\n",
    "    # Dtype of the cube representation (0-6 integers) and position representation (0-53 integers)\n",
    "    __dtype: torch.dtype = torch.long\n",
    "\n",
    "    # Move map for the cube\n",
    "    __MOVE_MAP = {\n",
    "        'U': (np.array([ 6,  7,  8,  5,  2,  1,  0,  3, 47, 50, 53, 29, 32, 35, 38, 41, 44, 20, 23, 26]),\n",
    "              np.array([ 0,  3,  6,  7,  8,  5,  2,  1, 20, 23, 26, 47, 50, 53, 29, 32, 35, 38, 41, 44])),\n",
    "        'D': (np.array([15, 12,  9, 10, 11, 14, 17, 16, 36, 39, 42, 18, 21, 24, 45, 48, 51, 27, 30, 33]),\n",
    "              np.array([ 9, 10, 11, 14, 17, 16, 15, 12, 18, 21, 24, 45, 48, 51, 27, 30, 33, 36, 39, 42])),\n",
    "        'R': (np.array([27, 28, 29, 32, 35, 34, 33, 30, 38, 37, 36, 15, 16, 17, 51, 52, 53,  6,  7,  8]),\n",
    "              np.array([29, 32, 35, 34, 33, 30, 27, 28, 15, 16, 17, 51, 52, 53,  6,  7,  8, 38, 37, 36])),\n",
    "        'L': (np.array([20, 23, 26, 25, 24, 21, 18, 19, 42, 43, 44,  2,  1,  0, 47, 46, 45, 11, 10,  9]),\n",
    "              np.array([26, 25, 24, 21, 18, 19, 20, 23,  2,  1,  0, 47, 46, 45, 11, 10,  9, 42, 43, 44])),\n",
    "        'F': (np.array([45, 46, 47, 50, 53, 52, 51, 48, 24, 25, 26,  0,  3,  6, 29, 28, 27, 17, 14, 11]),\n",
    "              np.array([47, 50, 53, 52, 51, 48, 45, 46,  0,  3,  6, 29, 28, 27, 17, 14, 11, 24, 25, 26])),\n",
    "        'B': (np.array([36, 37, 38, 41, 44, 43, 42, 39, 35, 34, 33, 15, 12,  9, 18, 19, 20,  2,  5,  8]),\n",
    "              np.array([38, 41, 44, 43, 42, 39, 36, 37,  2,  5,  8, 35, 34, 33, 15, 12,  9, 18, 19, 20])),\n",
    "    }\n",
    "    \n",
    "    # Faces and turn directions\n",
    "    __FACES: list[str] = [\"U\", \"D\", \"L\", \"R\", \"B\", \"F\"]\n",
    "\n",
    "    # Available rotation degrees\n",
    "    # Current only 90 degrees is supported\n",
    "    ## [90 degrees clockwise, 90 degrees counter-clockwise]\n",
    "    __DEGREES: list[str] = [\"\", \"'\"]\n",
    "\n",
    "    # Goal state of the cube\n",
    "    GOAL: torch.Tensor = torch.arange(0, 6 * 3 * 3, dtype=__dtype, device=device) // 9\n",
    "\n",
    "    ## Variables to be initialized ##\n",
    "\n",
    "    # Moves available for the cube\n",
    "    MOVES: list[str] = None\n",
    "\n",
    "    # Map of move names to indices\n",
    "    MOVE_TO_INDEX: dict[str, int] = None\n",
    "\n",
    "    # Source and target indices for the move map\n",
    "    MOVE_MAP_SOURCE: torch.Tensor = None\n",
    "    MOVE_MAP_TARGET: torch.Tensor = None\n",
    "\n",
    "    # The moves available for scrambling the cube\n",
    "    SCRAMBLE_MOVES_AVAILABLE: torch.Tensor = None\n",
    "\n",
    "    @classmethod\n",
    "    def init_class(cls, device=device) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the class variables.\n",
    "        Supposed to be called once when the class is first loaded.\n",
    "\n",
    "        Args:\n",
    "            device (str): Device to use for the tensor.\n",
    "        \"\"\"\n",
    "        # Check if the class has already been initialized\n",
    "        if cls.__initialized:\n",
    "            return\n",
    "        \n",
    "        # Initialize the moves available for the cube\n",
    "        cls.MOVES = [f\"{face}{degree}\" for degree in cls.__DEGREES for face in cls.__FACES]\n",
    "\n",
    "        # Initialize the move-to-index mapping\n",
    "        cls.MOVE_TO_INDEX = {move: i for i, move in enumerate(cls.MOVES)}\n",
    "\n",
    "        # Initialize the source and target indices for the move map\n",
    "        cls.MOVE_MAP_SOURCE = torch.tensor(np.array([cls.__MOVE_MAP[move[0]][0 if \"'\" not in move else 1] for move in cls.MOVES]), dtype=cls.__dtype, device=device)\n",
    "        cls.MOVE_MAP_TARGET = torch.tensor(np.array([cls.__MOVE_MAP[move[0]][1 if \"'\" not in move else 0] for move in cls.MOVES]), dtype=cls.__dtype, device=device)\n",
    "\n",
    "        # Initialize the scramble moves available for the cube\n",
    "        cls.SCRAMBLE_MOVES_AVAILABLE = torch.arange(len(cls.MOVES), dtype=cls.__dtype, device=device).repeat(len(cls.MOVES), 1)\n",
    "        exclude = ((torch.arange(len(cls.MOVES), device=device) + 6) % len(cls.MOVES)).unsqueeze(1)\n",
    "        cls.SCRAMBLE_MOVES_AVAILABLE = cls.SCRAMBLE_MOVES_AVAILABLE[cls.SCRAMBLE_MOVES_AVAILABLE != exclude].reshape(len(cls.MOVES), -1)\n",
    "\n",
    "        # Set the initialization flag to True\n",
    "        cls.__initialized = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def reverse_moves(moves: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Reverse the moves for the cube.\n",
    "\n",
    "        Args:\n",
    "            moves (torch.Tensor): Tensor of moves to reverse.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of reversed moves.\n",
    "        \"\"\"\n",
    "        # Reverse the moves and apply the inverse mapping\n",
    "        return (moves + 6) % 12\n",
    "\n",
    "    def __init__(self, tensor: torch.Tensor | None = None, num_cubes: int | None = 1, device: str | None = device):\n",
    "        \"\"\"\n",
    "        Initialize the Cubes object.\n",
    "\n",
    "        Args:\n",
    "            tensor (torch.Tensor): Tensor representation of the cubes.\n",
    "            num_cubes (int): Number of cubes to initialize.\n",
    "            device (str): Device to use for the tensor.\n",
    "        \"\"\"\n",
    "        # Call the class initialization method\n",
    "        self.init_class(device=device)\n",
    "\n",
    "        # Set the tensor for the cubes\n",
    "        if tensor is None:\n",
    "            # Check if num_cubes and device are provided\n",
    "            if num_cubes is None or device is None:\n",
    "                raise ValueError(\"Either tensor or both num_cubes and device must be provided\")\n",
    "            \n",
    "            # Create num_cubes cubes in the goal state on the specified device\n",
    "            self.reset(num_cubes=num_cubes, device=device)\n",
    "\n",
    "        else:\n",
    "            # Ignore the num_cubes and device arguments if tensor is provided\n",
    "            self.tensor = tensor\n",
    "\n",
    "            # Verify the tensor shape\n",
    "            if (self.tensor.ndim == 1):\n",
    "                self.tensor = self.tensor.unsqueeze(0)\n",
    "\n",
    "            if (self.tensor.ndim != 2) or (self.tensor.shape[1] != 6 * 3 * 3):\n",
    "                raise ValueError(\"Tensor must be of shape (num_cubes, 6 * 3 * 3)\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of cubes.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of cubes.\n",
    "        \"\"\"\n",
    "        return self.tensor.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Get a specific cube by index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the cube.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The cube at the specified index.\n",
    "        \"\"\"\n",
    "        return self.tensor[index]\n",
    "    \n",
    "    def __setitem__(self, index: int, value: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Set a specific cube by index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the cube.\n",
    "            value (torch.Tensor): New value for the cube.\n",
    "        \"\"\"\n",
    "        if value.shape != (6 * 3 * 3,):\n",
    "            raise ValueError(\"Value must be of shape (6 * 3 * 3)\")\n",
    "        self.tensor[index] = value.to(self.tensor.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Get a string representation of the cubes.\n",
    "\n",
    "        Returns:\n",
    "            str: String representation of the cubes.\n",
    "        \"\"\"\n",
    "        return f\"Cubes(tensor={self.tensor})\"\n",
    "    \n",
    "    def to(self, device: str) -> None:\n",
    "    \n",
    "        \"\"\"\n",
    "        Move the cubes to the specified device.\n",
    "\n",
    "        Args:\n",
    "            device (str): Device to move the tensor to.\n",
    "        \"\"\"\n",
    "        self.tensor = self.tensor.to(device)\n",
    "\n",
    "    def reset(self, num_cubes: int | None = None, device: str | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Reset the cubes to the goal state.\n",
    "\n",
    "        Args:\n",
    "            num_cubes (int): Number of cubes to reset.\n",
    "            device (str): Device to use for the tensor.\n",
    "        \"\"\"\n",
    "        # Set parameters to default values if not provided\n",
    "        if num_cubes is None:\n",
    "            num_cubes = self.tensor.shape[0]\n",
    "        if device is None:\n",
    "            device = self.tensor.device\n",
    "        \n",
    "        # Move the goal state to the specified device\n",
    "        self.GOAL = self.GOAL.to(device)\n",
    "\n",
    "        # Create num_cubes cubes in the goal state on the specified device\n",
    "        self.tensor = self.GOAL.unsqueeze(0).repeat(num_cubes, 1)\n",
    "\n",
    "    def is_solved(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Check if the cubes are in the solved state.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Boolean tensor indicating if each cube is solved.\n",
    "        \"\"\"\n",
    "        return (self.tensor == self.GOAL).all(dim=1)\n",
    "    \n",
    "    def move(self, moves: str | list[str] | list[list[str]] | int | list[int] | list[list[int]] | torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Apply a single or a sequence of moves to the cubes.\n",
    "\n",
    "        Args:\n",
    "            move (str): Move to apply.\n",
    "        \"\"\"\n",
    "        # Convert the parameter to Tensor\n",
    "\n",
    "        # str -> int\n",
    "        if isinstance(moves, str):\n",
    "            return self.move(self.MOVE_TO_INDEX[moves])\n",
    "        \n",
    "        # list[str] -> list[int]\n",
    "        elif isinstance(moves, list) and all(isinstance(move, str) for move in moves):\n",
    "            if len(moves) != self.tensor.shape[0]:\n",
    "                raise ValueError(\"Length of move list must match the number of cubes\")\n",
    "            return self.move([self.MOVE_TO_INDEX[move] for move in moves])\n",
    "        \n",
    "        # list[list[str]] -> list[list[int]]\n",
    "        elif isinstance(moves, list) and all(isinstance(step, list) and all(isinstance(move, str) for move in step) for step in moves):\n",
    "            if any(len(moves[i]) != self.tensor.shape[0] for i in range(len(moves))):\n",
    "                raise ValueError(\"Length of move list must match the number of cubes\")\n",
    "            return self.move([[self.MOVE_TO_INDEX[move] for move in round] for round in moves])\n",
    "        \n",
    "        # int -> Tensor\n",
    "        elif isinstance(moves, int):\n",
    "            moves = torch.full((self.tensor.shape[0],), moves, dtype=self.__dtype, device=self.tensor.device)\n",
    "            return self.move(moves)\n",
    "        \n",
    "        # list[int] -> Tensor\n",
    "        elif isinstance(moves, list) and all(isinstance(move, int) for move in moves):\n",
    "            if len(moves) != self.tensor.shape[0]:\n",
    "                raise ValueError(\"Length of move list must match the number of cubes\")\n",
    "            moves = torch.tensor(moves, dtype=self.__dtype, device=self.tensor.device)\n",
    "            return self.move(moves)\n",
    "        \n",
    "        # list[list[int]] -> Tensor\n",
    "        elif isinstance(moves, list) and all(isinstance(step, list) and all(isinstance(move, int) for move in step) for step in moves):\n",
    "            if any(len(moves[i]) != self.tensor.shape[0] for i in range(len(moves))):\n",
    "                raise ValueError(\"Length of move list must match the number of cubes\")\n",
    "            moves = torch.tensor(moves, dtype=self.__dtype, device=self.tensor.device)\n",
    "            return self.move(moves)\n",
    "        \n",
    "        # 2D Tensor -> Tensor\n",
    "        elif isinstance(moves, torch.Tensor) and moves.ndim == 2 and moves.shape[1] == self.tensor.shape[0]:\n",
    "            moves = moves.to(self.tensor.device)\n",
    "            for i in range(moves.shape[0]):\n",
    "                self.move(moves[i])\n",
    "            return\n",
    "        \n",
    "        # 1D Tensor -> Tensor\n",
    "        elif isinstance(moves, torch.Tensor) and moves.ndim == 1:\n",
    "            if moves.shape[0] != self.tensor.shape[0]:\n",
    "                raise ValueError(\"Length of move list must match the number of cubes\")\n",
    "            self.__move_torch(moves)  # Implement move for cubes using the __move_torch method\n",
    "            \n",
    "        # Other types\n",
    "        else:\n",
    "            raise ValueError(\"Invalid move type or shape.\")\n",
    "\n",
    "    def __move_torch(self, move: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Apply a move to the cubes using PyTorch.\n",
    "\n",
    "        Args:\n",
    "            move (torch.Tensor): Tensor of moves to apply.\n",
    "        \"\"\"\n",
    "        move = move.to(self.tensor.device)\n",
    "\n",
    "        # Source indices for the move map\n",
    "        self.MOVE_MAP_SOURCE = self.MOVE_MAP_SOURCE.to(self.tensor.device)\n",
    "        source_idx = self.MOVE_MAP_SOURCE[move]\n",
    "\n",
    "        # Target indices for the move map\n",
    "        self.MOVE_MAP_TARGET = self.MOVE_MAP_TARGET.to(self.tensor.device)\n",
    "        target_idx = self.MOVE_MAP_TARGET[move]\n",
    "\n",
    "        # Batch indices for the cubes\n",
    "        batch_idx = torch.arange(self.tensor.shape[0], device=self.tensor.device).unsqueeze(1)\n",
    "\n",
    "        # Apply the move to the cubes\n",
    "        self.tensor[batch_idx, target_idx] = self.tensor[batch_idx, source_idx]\n",
    "\n",
    "    def scramble(self, scramble_length: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate a random scramble for the cubes.\n",
    "\n",
    "        Args:\n",
    "            scramble_length (int): Length of the scramble.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of moves for the scramble.\n",
    "        \"\"\"\n",
    "        # Generate a random scramble\n",
    "        plan = self.plan_scramble(scramble_length)\n",
    "        self.move(plan)\n",
    "\n",
    "        return plan\n",
    "\n",
    "    def plan_scramble(self, scramble_length: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate a random scramble plan.\n",
    "\n",
    "        Args:\n",
    "            scramble_length (int): Length of the scramble.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of moves for the scramble plan.\n",
    "        \"\"\"\n",
    "        # Move the needed variables to the device\n",
    "        self.SCRAMBLE_MOVES_AVAILABLE = self.SCRAMBLE_MOVES_AVAILABLE.to(self.tensor.device)\n",
    "\n",
    "        # Generate a random scramble plan\n",
    "        plan = torch.empty((scramble_length, self.tensor.shape[0]), dtype=self.__dtype, device=self.tensor.device)\n",
    "        for i in range(scramble_length):\n",
    "            if i == 0:\n",
    "                # The initial move is chosen randomly from the available moves\n",
    "                plan[i] = torch.randint(0, len(self.MOVES), (self.tensor.shape[0],), dtype=self.__dtype, device=self.tensor.device)\n",
    "            elif i == 1:\n",
    "                # The second move is chosen randomly from the available moves, excluding the inverse of the first move\n",
    "                plan[i] = torch.randint(0, self.SCRAMBLE_MOVES_AVAILABLE.shape[1], (self.tensor.shape[0],), dtype=self.__dtype, device=self.tensor.device)\n",
    "                plan[i] = self.SCRAMBLE_MOVES_AVAILABLE[plan[i - 1], plan[i]]\n",
    "            else:\n",
    "                generate_idx = torch.arange(self.tensor.shape[0], device=self.tensor.device)\n",
    "                while generate_idx.shape[0] > 0:\n",
    "                    # Choose a random move from the available moves, excluding the inverse of the previous move\n",
    "                    plan[i, generate_idx] = torch.randint(0, self.SCRAMBLE_MOVES_AVAILABLE.shape[1], (generate_idx.shape[0],), dtype=self.__dtype, device=self.tensor.device)\n",
    "                    plan[i, generate_idx] = self.SCRAMBLE_MOVES_AVAILABLE[plan[i - 1, generate_idx], plan[i, generate_idx]]\n",
    "\n",
    "                    # We use a range of 2 for redundancy checking\n",
    "                    # Prevent three consecutive moves from being the same -> Can be replaced with a single move\n",
    "                    # e.g. U U (U) -> U'\n",
    "                    mask1 = (plan[i, generate_idx] == plan[i - 1, generate_idx]) & (plan[i - 1, generate_idx] == plan[i - 2, generate_idx])\n",
    "\n",
    "                    # Prevent two mutually canceling moves sandwiching an opposite face move\n",
    "                    # e.g. U D (U') -> D\n",
    "                    mask2 = (self.reverse_moves(plan[i, generate_idx]) == plan[i - 2, generate_idx]) & ((plan[i, generate_idx] // 2) % 3 == (plan[i - 1, generate_idx] // 2) % 3) & (plan[i, generate_idx] % 6 != plan[i - 1, generate_idx] % 6)\n",
    "\n",
    "                    # Continue if there are no invalid moves\n",
    "                    generate_idx = generate_idx[mask1 | mask2]\n",
    "\n",
    "        return plan\n",
    "\n",
    "Cubes.init_class(device=device)  # Initialize the Cubes class with the specified device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7708e",
   "metadata": {},
   "source": [
    "## Model\n",
    "This section defines the model used to predict the last move of the scrambling path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ede4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with ReLU and BatchNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, input_prev, embed_dim):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_prev, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layers(inputs)\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with two linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            LinearBlock(embed_dim, embed_dim),\n",
    "            LinearBlock(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.layers(x)\n",
    "        x += inputs # skip-connection\n",
    "        return x\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Fixed architecture following DeepCubeA.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=6*3*3*6, output_dim=len(Cubes.MOVES)):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            LinearBlock(input_dim, 5000),\n",
    "            LinearBlock(5000,1000),\n",
    "            ResidualBlock(1000),\n",
    "            ResidualBlock(1000),\n",
    "            ResidualBlock(1000),\n",
    "            ResidualBlock(1000),\n",
    "            nn.Linear(1000, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # int indices => float one-hot vectors\n",
    "        x = F.one_hot(inputs, num_classes=6).to(torch.float)\n",
    "        x = x.reshape(-1, self.input_dim)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "model = Model()\n",
    "model = model.to(device).compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d575f",
   "metadata": {},
   "source": [
    "## Training\n",
    "In this section, the model is training using real-time generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c34b9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dummy dataset to drive the training loop.\n",
    "\n",
    "    The dataset generation logic is implemented in the collate_fn function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            total_samples = TrainConfig.batch_size_per_depth * TrainConfig.num_steps\n",
    "        ):\n",
    "        self.total_samples = total_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the index only\n",
    "        return idx\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to generate a batch of data.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of indices from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the input and target tensors.\n",
    "    \"\"\"\n",
    "    # Generate random cubes and moves\n",
    "    num_cubes = len(batch)\n",
    "    cubes = Cubes(num_cubes=num_cubes, device=device)\n",
    "\n",
    "    # Prepare output data\n",
    "    tensor = torch.empty(\n",
    "        (TrainConfig.max_depth * num_cubes, 6 * 3 * 3), dtype=torch.long, device=device\n",
    "    )\n",
    "\n",
    "    # Generate cubes for each depth\n",
    "    plan = cubes.plan_scramble(TrainConfig.max_depth)\n",
    "    for i in range(plan.shape[0]):\n",
    "        # Apply the moves to the cubes\n",
    "        cubes.move(plan[i])\n",
    "\n",
    "        # Store the cubes in the tensor\n",
    "        tensor[i * num_cubes : (i + 1) * num_cubes] = cubes.tensor\n",
    "\n",
    "    # Generate target moves\n",
    "    moves = plan.flatten().to(device)\n",
    "\n",
    "    return tensor, moves\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    EchoDataset(),\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=TrainConfig.batch_size_per_depth,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(losses):\n",
    "    \"\"\"\n",
    "    Plot the loss curve.\n",
    "    \n",
    "    Args:\n",
    "        h (list): List of loss values.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(losses)\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Cross-entropy loss\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    plt.show()\n",
    "\n",
    "def train(model: Model, dataloader: torch.utils.data.DataLoader) -> Model:\n",
    "    \"\"\"\n",
    "    Train the model on the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (Model): The model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    \"\"\"\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=TrainConfig.learning_rate)\n",
    "\n",
    "    # Data generator\n",
    "    loop = tqdm(dataloader, unit=\"batch\")\n",
    "\n",
    "    # Training losses\n",
    "    losses = []\n",
    "\n",
    "    # Context manager for mixed precision training\n",
    "    ctx = torch.amp.autocast('cuda', dtype=torch.float16) if TrainConfig.ENABLE_FP16 else nullcontext()\n",
    "\n",
    "    # TODO: Change steps to epochs\n",
    "    for batch_index, (batch_x, batch_y) in enumerate(loop):\n",
    "        # Adjust data shape for the model\n",
    "        batch_x, batch_y = batch_x.reshape(-1, 54).to(device), batch_y.reshape(-1).to(device)\n",
    "\n",
    "        # Training step\n",
    "        with ctx:\n",
    "            pred_y = model(batch_x)\n",
    "            loss = loss_fn(pred_y, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update losses and progress bar\n",
    "        losses.append(loss.item())\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Plot the loss curve\n",
    "        # Plot the loss curve every INTERVAL_PLOT steps\n",
    "        if TrainConfig.INTERVAL_PLOT and (batch_index+1) % TrainConfig.INTERVAL_PLOT == 0:\n",
    "            clear_output()\n",
    "            plot_loss_curve(losses)\n",
    "\n",
    "        # Save the model\n",
    "        # Save the model every INTERVAL_SAVE steps\n",
    "        if TrainConfig.INTERVAL_SAVE and (batch_index+1) % TrainConfig.INTERVAL_SAVE == 0:\n",
    "            torch.save(model.state_dict(), f\"{batch_index+1}steps.pth\")\n",
    "            print(\"Model saved.\")\n",
    "\n",
    "    print(f\"Trained on data equivalent to {TrainConfig.batch_size_per_depth * TrainConfig.num_steps} solves.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = train(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516ad15",
   "metadata": {},
   "source": [
    "## Inference\n",
    "We test and comapare our model on the DeepCubeA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0781e",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Retrieve the data set from GitHub if not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d36b86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haoti\\code\\uiuc\\ece448\\project\\UIUC-ECE448-Project-SP25\\DeepCubeA\n",
      "### Optimal Solver ###\n",
      "dict_keys(['states', 'times', 'solutions', 'num_nodes_generated'])\n",
      "No. of cases: 1000\n",
      "\n",
      "### DeepCubeA ###\n",
      "dict_keys(['states', 'solutions', 'paths', 'times', 'num_nodes_generated'])\n",
      "No. of cases: 1000\n",
      "c:\\Users\\haoti\\code\\uiuc\\ece448\\project\\UIUC-ECE448-Project-SP25\n"
     ]
    }
   ],
   "source": [
    "# Download the DeepCubeA repository if not already present\n",
    "if \"DeepCubeA\" != os.getcwd().split(\"/\")[-1]:\n",
    "    if not os.path.exists(\"DeepCubeA\"):\n",
    "        !git clone -q https://github.com/forestagostinelli/DeepCubeA\n",
    "    %cd ./DeepCubeA/\n",
    "\n",
    "# Load the data set\n",
    "print('### Optimal Solver ###')\n",
    "filename = 'data/cube3/test/data_0.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    result_Optimal = pickle.load(f)\n",
    "\n",
    "    print(result_Optimal.keys())\n",
    "    result_Optimal[\"solution_lengths\"] = [len(s) for s in result_Optimal[\"solutions\"]]\n",
    "    result_Optimal[\"solution_lengths_count\"] = {\n",
    "        i: result_Optimal[\"solution_lengths\"].count(i)\n",
    "        for i in range(min(result_Optimal[\"solution_lengths\"]), max(result_Optimal[\"solution_lengths\"]))\n",
    "    }\n",
    "\n",
    "    print('No. of cases:', len(result_Optimal[\"solution_lengths\"]))\n",
    "\n",
    "# Load the result of DeepCubeA for comparison\n",
    "print('\\n### DeepCubeA ###')\n",
    "filename = 'results/cube3/results.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    result_DeepCubeA = pickle.load(f)\n",
    "\n",
    "    print(result_DeepCubeA.keys())\n",
    "    result_DeepCubeA[\"solution_lengths\"] = [len(s) for s in result_DeepCubeA[\"solutions\"]]\n",
    "    result_DeepCubeA[\"solution_lengths_count\"] = {\n",
    "        i: result_DeepCubeA[\"solution_lengths\"].count(i)\n",
    "        for i in range(min(result_DeepCubeA[\"solution_lengths\"]), max(result_DeepCubeA[\"solution_lengths\"]))\n",
    "    }\n",
    "\n",
    "    print('No. of cases:', len(result_DeepCubeA[\"solution_lengths\"]))\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fca6a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "[['D', -1], ['F', 1], ['R', 1], ['U', -1], ['F', 1], ['F', 1], ['R', 1], ['U', 1], ['F', 1], ['R', 1], ['B', -1], ['R', -1], ['F', -1], ['R', -1], ['D', -1], ['U', -1], ['R', -1], ['U', -1], ['U', -1], ['R', -1], ['U', 1], ['B', -1]]\n",
      "-> ['B', \"U'\", 'R', 'U', 'U', 'R', 'U', 'D', 'R', 'F', 'R', 'B', \"R'\", \"F'\", \"U'\", \"R'\", \"F'\", \"F'\", 'U', \"R'\", \"F'\", 'D']\n"
     ]
    }
   ],
   "source": [
    "# Convert optimal solutions to test scrambles\n",
    "def solution2scramble(solution):\n",
    "    return [m[0] if m[1] == -1 else m[0] + \"'\" for m in solution[::-1]]\n",
    "\n",
    "test_scrambles = [solution2scramble(s) for s in result_Optimal[\"solutions\"]]\n",
    "\n",
    "print(f\"\"\"Example:\\n{result_Optimal[\"solutions\"][0]}\\n-> {test_scrambles[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013b36e",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "We use beam search to expand the traced set of possible solutions, which does not guarantee to give a solution but effiectively improved the probability of finding one (and the one as optimial as possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def beam_search(\n",
    "    cubes: Cubes, \n",
    "    model: Model, \n",
    "    beam_width: int = SearchConfig.beam_width, \n",
    "    max_depth: int = SearchConfig.max_depth,\n",
    "    skip_redundant_moves: bool = True   # FIXME: This is not implemented yet\n",
    "    ) -> list[None | dict]:\n",
    "    \"\"\"\n",
    "    Best-first beam search for the optimal solution.\n",
    "    \n",
    "    Args:\n",
    "        cubes (Cubes): The cubes to be solved.\n",
    "        model (Model): The model to be used for prediction.\n",
    "        beam_width (int): The width of the beam search.\n",
    "        max_depth (int): The maximum depth of the search.\n",
    "        skip_redundant_moves (bool): Whether to skip redundant moves.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The predicted moves for the solution.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare the data structure for the beam search\n",
    "    candidates = torch.empty((len(cubes), beam_width, 6 * 3 * 3), dtype=torch.long, device=cubes.tensor.device)\n",
    "    candidates[:, 0] = cubes.tensor\n",
    "    candidate_paths = torch.empty((len(cubes), beam_width, max_depth), dtype=torch.long, device=candidates.device)\n",
    "    candidate_log_probs = torch.zeros((len(cubes), beam_width), dtype=torch.float, device=candidates.device)\n",
    "    candidate_cube_idx = torch.arange(len(cubes), device=candidates.device).unsqueeze(1).expand(-1, beam_width)\n",
    "\n",
    "    # Prepare the data structure for output\n",
    "    output = [None] * len(cubes)\n",
    "    time_0 = time.time()\n",
    "\n",
    "    # Initialize the beam search\n",
    "    for depth in range(max_depth):\n",
    "        # Select the candidates for the current depth\n",
    "        candidate_len = min(beam_width, len(Cubes.MOVES)**depth)\n",
    "        active_candidates = candidates[:, :candidate_len].reshape(-1, 6 * 3 * 3)\n",
    "        active_candidate_paths = candidate_paths[:, :candidate_len, :depth].reshape(candidate_paths.shape[0] * candidate_len, depth)\n",
    "        active_candidate_cube_idx = candidate_cube_idx[:, :candidate_len].flatten()\n",
    "\n",
    "        # Check if the candidates are already solved\n",
    "        solved_mask = Cubes(active_candidates).is_solved()\n",
    "        if solved_mask.any():\n",
    "            # If any of the candidates are solved, update the output\n",
    "            solved_active_candidate_idx = torch.arange(solved_mask.shape[0], device=candidates.device)[solved_mask]\n",
    "            solved_cube_idx = active_candidate_cube_idx[solved_mask]\n",
    "            time_duration = time.time() - time_0\n",
    "            for i in range(solved_active_candidate_idx.shape[0]):\n",
    "                solved_idx = solved_active_candidate_idx[i]\n",
    "                cube_idx = solved_cube_idx[i]\n",
    "                if output[cube_idx] is None:\n",
    "                    output[cube_idx] = {\n",
    "                        \"solution\": active_candidate_paths[solved_idx].cpu().numpy().tolist(),\n",
    "                        \"time\": time_duration,\n",
    "                        \"depth\": depth,\n",
    "                    }\n",
    "\n",
    "            # If all cubes are solved, break the loop\n",
    "            solved_cube_idx = solved_cube_idx.unique()\n",
    "            if solved_cube_idx.shape[0] == candidates.shape[0]:\n",
    "                break\n",
    "            \n",
    "            # Remove the solved cubes from the candidates\n",
    "            cube_mask = torch.ones(candidates.shape[0], dtype=torch.bool, device=candidates.device)\n",
    "            cube_mask[solved_cube_idx.unique()] = False\n",
    "            candidates = candidates[cube_mask]\n",
    "            candidate_paths = candidate_paths[cube_mask]\n",
    "            candidate_log_probs = candidate_log_probs[cube_mask]\n",
    "            candidate_cube_idx = candidate_cube_idx[cube_mask]\n",
    "\n",
    "            # Regenerate the active candidates\n",
    "            active_candidates = candidates[:, :candidate_len].reshape(-1, 6 * 3 * 3)\n",
    "            active_candidate_paths = candidate_paths[:, :candidate_len, :depth].reshape(candidate_paths.shape[0] * candidate_len, depth)\n",
    "            active_candidate_cube_idx = candidate_cube_idx[:, :candidate_len].flatten()\n",
    "            \n",
    "        # Get the predictions from the model\n",
    "        pred = model(active_candidates).reshape(candidates.shape[0], candidate_len, -1)\n",
    "\n",
    "        # Calculate the log probabilities\n",
    "        log_probs = F.log_softmax(pred, dim=-1)\n",
    "\n",
    "        # Filter the log probabilities based on the active candidates\n",
    "        active_candidate_log_probs = candidate_log_probs[:, :candidate_len]\n",
    "        next_move_log_probs = active_candidate_log_probs.unsqueeze(-1) + log_probs\n",
    "        next_move_log_probs = next_move_log_probs.reshape(next_move_log_probs.shape[0], -1)\n",
    "        next_moves = torch.arange(log_probs.shape[-1], device=candidates.device).repeat(candidate_len).unsqueeze(0).expand(next_move_log_probs.shape[0], -1)\n",
    "        next_move_candidate_idx = torch.arange(candidate_len, device=candidates.device).unsqueeze(-1).repeat(1, log_probs.shape[-1]).flatten().unsqueeze(0).expand(next_move_log_probs.shape[0], -1)\n",
    "\n",
    "        # Remove redundant moves if specified\n",
    "        if skip_redundant_moves and depth > 0:\n",
    "            # Build the mask for redundant moves\n",
    "            last_moves = active_candidate_paths[:, -1].reshape(candidate_paths.shape[0], candidate_len, 1).expand(-1, -1, log_probs.shape[-1]).reshape(next_move_log_probs.shape[0], -1)\n",
    "            assert last_moves.shape == next_moves.shape, f\"last_moves: {last_moves.shape}, next_moves: {next_moves.shape}\"\n",
    "            mask = Cubes.reverse_moves(last_moves) == next_moves\n",
    "            next_move_log_probs = next_move_log_probs.masked_fill(mask, -float(\"inf\"))\n",
    "        \n",
    "        # Filter next moves based on probabilities\n",
    "        sorted_next_move_log_probs_idx = torch.argsort(next_move_log_probs, dim=-1, descending=True)[:, :beam_width]\n",
    "        next_move_log_probs = next_move_log_probs.gather(1, sorted_next_move_log_probs_idx)\n",
    "        next_moves = next_moves.gather(1, sorted_next_move_log_probs_idx)\n",
    "        next_move_candidate_idx = next_move_candidate_idx.gather(1, sorted_next_move_log_probs_idx)\n",
    "\n",
    "        # Update the candidates with the next moves\n",
    "        candidates[:, :next_move_candidate_idx.shape[1]] = candidates.gather(1, next_move_candidate_idx.unsqueeze(-1).expand(-1, -1, 6 * 3 * 3))\n",
    "        candidate_paths[:, :next_move_candidate_idx.shape[1], :depth] = active_candidate_paths.reshape(candidate_paths.shape[0], candidate_len, depth).gather(1, next_move_candidate_idx.unsqueeze(-1).expand(-1, -1, depth))\n",
    "        candidate_log_probs[:, :next_move_candidate_idx.shape[1]] = next_move_log_probs\n",
    "        \n",
    "        # Apply the next moves to the candidates\n",
    "        candidate_paths[:, :next_move_candidate_idx.shape[1], depth] = next_moves\n",
    "        temp_cubes = Cubes(candidates[:, :next_move_candidate_idx.shape[1]].reshape(-1, 6 * 3 * 3))\n",
    "        temp_cubes.move(Cubes.reverse_moves(next_moves.flatten()))\n",
    "        candidates[:, :next_move_candidate_idx.shape[1]] = temp_cubes.tensor.reshape(candidates.shape[0], -1, 6 * 3 * 3).clone()\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = Cubes(num_cubes=1)\n",
    "cube.move([[move] for move in test_scrambles[1]])\n",
    "output = beam_search(cube, model, beam_width=2*11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f971c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.move(Cubes.reverse_moves(torch.tensor([[move] for move in output[0][\"solution\"]])))\n",
    "cube.is_solved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = iter(torch.utils.data.DataLoader(\n",
    "    EchoDataset(),\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=2,\n",
    "))\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize(cubes):\n",
    "    grid = [[-1, -1, -1,  2, 5, 8,  -1, -1, -1,  -1, -1, -1],\n",
    "            [-1, -1, -1,  1, 4, 7,  -1, -1, -1,  -1, -1, -1],\n",
    "            [-1, -1, -1,  0, 3, 6,  -1, -1, -1,  -1, -1, -1],\n",
    "            [20, 23, 26,  47, 50, 53,  29, 32, 35,  38, 41, 44],\n",
    "            [19, 22, 25,  46, 49, 52,  28, 31, 34,  37, 40, 43],\n",
    "            [18, 21, 24,  45, 48, 51,  27, 30, 33,  36, 39, 42],\n",
    "            [-1, -1, -1,  11, 14, 17,  -1, -1, -1,  -1, -1, -1],\n",
    "            [-1, -1, -1,  10, 13, 16,  -1, -1, -1,  -1, -1, -1],\n",
    "            [-1, -1, -1,  9, 12, 15,  -1, -1, -1,  -1, -1, -1]]\n",
    "    fig, axes = plt.subplots(cubes.shape[0], 1, figsize=(12, 8 * cubes.shape[0]), dpi=100)\n",
    "    for cube_idx, ax in enumerate(axes):\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlim(0, 12)\n",
    "        ax.set_ylim(-8, 1)\n",
    "        plt.axis('off')\n",
    "        for i in range(9):\n",
    "            for j in range(12):\n",
    "                if grid[i][j] == -1:\n",
    "                    continue\n",
    "                color = {0: 'yellow', 1: 'white', 2: 'blue',\n",
    "                        3: 'green', 4: 'orange', 5: 'red'}[cubes[cube_idx, grid[i][j]].item()]\n",
    "                square = patches.Rectangle(\n",
    "                    (j, -i), 1, 1, edgecolor='black', facecolor=color)\n",
    "                ax.add_patch(square)\n",
    "    plt.show()\n",
    "cube = Cubes(num_cubes=2)\n",
    "cube.move([\"U\", \"U'\"])\n",
    "visualize(cube.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8dd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search(cube, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "877d0b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9506, -1.4207,  0.2974,  1.0400, -1.1955],\n",
       "         [ 0.7597, -0.1511, -1.4915, -0.0370,  0.4546]],\n",
       "\n",
       "        [[-0.4683,  1.4906,  0.4209, -0.8778,  0.3123],\n",
       "         [-0.4545, -0.3256,  0.2490,  1.0260,  0.2440]],\n",
       "\n",
       "        [[ 0.2344,  1.6269,  1.6757,  0.7930,  0.8204],\n",
       "         [ 1.3450, -0.2901,  1.6133, -0.4839,  0.3678]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4 , 5)[:, :2].reshape(-1, 5).reshape(torch.randn(3, 4 , 5)[:, :2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbd22d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0636, -0.7576],\n",
       "         [-1.6166,  0.8245],\n",
       "         [-1.8340, -1.8919]],\n",
       "\n",
       "        [[-1.0256, -1.0919],\n",
       "         [ 0.4804,  0.6505],\n",
       "         [-0.5176,  0.1917]],\n",
       "\n",
       "        [[-0.4464,  0.7612],\n",
       "         [-0.2182,  0.5445],\n",
       "         [ 1.9417, -0.8868]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4, 5).gather(1, torch.tensor([[0, 1], [2, 3], [1, 2]]).unsqueeze(0).expand(3, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes = Cubes(num_cubes=12)\n",
    "cubes.move(Cubes.MOVES)\n",
    "cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f984dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes.is_solved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9906948",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_moves = Cubes.reverse_moves(np.array([Cubes.MOVE_TO_INDEX[m] for m in Cubes.MOVES]))\n",
    "cubes.move(reverse_moves.tolist())\n",
    "cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes.is_solved()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
